{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccf1c7c-2f60-47d8-a64c-b2a5850f5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa10ae2a-abfd-4548-89a3-b38a30196229",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(':memory:') \n",
    "cursor = conn.cursor()\n",
    "cursor.execute('CREATE TABLE students (id INTEGER, name TEXT, grade INTEGER)')\n",
    "data = [(1, 'Alice', 85), (2, 'Bob', 90), (3, 'Charlie', 78), (4, 'David', 92)]\n",
    "cursor.executemany('INSERT INTO students VALUES (?,?,?)', data)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5946aa64-0acc-4040-bbd0-37b969532f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id     name  grade\n",
      "0   1    Alice     85\n",
      "1   2      Bob     90\n",
      "2   3  Charlie     78\n",
      "3   4    David     92\n"
     ]
    }
   ],
   "source": [
    "# data collection from a database\n",
    "query = \"SELECT * FROM students\"\n",
    "df_db = pd.read_sql_query(query, conn)\n",
    "print(df_db)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4208649-55ee-4ab8-9ab0-a46587fac43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id              name                      email\n",
      "0   1     Leanne Graham          Sincere@april.biz\n",
      "1   2      Ervin Howell          Shanna@melissa.tv\n",
      "2   3  Clementine Bauch         Nathan@yesenia.net\n",
      "3   4  Patricia Lebsack  Julianne.OConner@kory.org\n",
      "4   5  Chelsey Dietrich   Lucio_Hettinger@annie.ca\n"
     ]
    }
   ],
   "source": [
    "# data collection from an API\n",
    "\n",
    "api_url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "response = requests.get(api_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    json_data = response.json()\n",
    "    df_api = pd.DataFrame(json_data)\n",
    "    df_api_clean = df_api[['id', 'name', 'email']]\n",
    "    print(df_api_clean.head())\n",
    "else:\n",
    "    print(\"Failed to fetch API data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1da7c0-c390-4786-bdd5-14395d26ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Temp\n",
      "0 1981-01-01  20.7\n",
      "1 1981-01-02  17.9\n",
      "2 1981-01-03  18.8\n",
      "3 1981-01-04  14.6\n",
      "4 1981-01-05  15.8\n"
     ]
    }
   ],
   "source": [
    "# data colletion from a sensor\n",
    "\n",
    "sensor_data_url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "\n",
    "try:\n",
    "    df_sensor = pd.read_csv(sensor_data_url, parse_dates=['Date'])\n",
    "    print(df_sensor.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0171fa44-75b6-4131-9e39-68dfb452016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Author                                              Quote\n",
      "0  Albert Einstein  “The world as we have created it is a process ...\n",
      "1     J.K. Rowling  “It is our choices, Harry, that show what we t...\n",
      "2  Albert Einstein  “There are only two ways to live your life. On...\n",
      "3      Jane Austen  “The person, be it gentleman or lady, who has ...\n",
      "4   Marilyn Monroe  “Imperfection is beauty, madness is genius and...\n"
     ]
    }
   ],
   "source": [
    "# data collection by web scraping\n",
    "\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "\n",
    "try:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # The site holds quotes in <span class=\"text\"> and authors in <small class=\"author\">\n",
    "    quotes = soup.find_all('span', class_='text')\n",
    "    authors = soup.find_all('small', class_='author')\n",
    "\n",
    "    scraped_data = []\n",
    "    for quote, author in zip(quotes, authors):\n",
    "        scraped_data.append({\n",
    "            'Author': author.text,\n",
    "            'Quote': quote.text\n",
    "        })\n",
    "\n",
    "    df_scraping = pd.DataFrame(scraped_data)\n",
    "    print(df_scraping.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d8f40c-c585-4aa3-aab4-ef33cf6f55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download. Status Code: 404\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# collecting an image\n",
    "\n",
    "# Using a standard data science sample image (more reliable than Wikipedia for testing)\n",
    "image_url = \"https://raw.githubusercontent.com/scikit-image/scikit-image/main/skimage/data/chelsea.png\"\n",
    "\n",
    "try:\n",
    "    # We use a specific header to look like a real browser\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "    response = requests.get(image_url, headers=headers, stream=True)\n",
    "    \n",
    "    # Check if the download was actually successful (Code 200)\n",
    "    if response.status_code == 200:\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        print(f\"Image Shape: {img_array.shape}\")\n",
    "        print(f\"Data Type: {img_array.dtype}\")\n",
    "        \n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img_array)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Failed to download. Status Code: {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "487fce87-9ec0-4abd-bbd8-799db83050f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Live Sensor Stream...\n",
      "\n",
      "--- Live Sensor Update: 12:37:44 ---\n",
      "Total events detected in past hour: 19\n",
      "#1: Magnitude 2.69 - Location: 20 km NNE of Indio, CA\n",
      "#2: Magnitude 0.83 - Location: 21 km NNE of Indio, CA\n",
      "#3: Magnitude 1.61 - Location: 18 km N of Indio, CA\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# collecting data from a live sensor\n",
    "\n",
    "# URL for USGS live earthquake sensor feed (GeoJSON format)\n",
    "# This feed contains all earthquakes recorded in the last hour\n",
    "URL = \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson\"\n",
    "\n",
    "def get_live_sensor_data():\n",
    "    try:\n",
    "        # 1. Extract: Send a GET request to the cloud endpoint\n",
    "        response = requests.get(URL)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # 2. Parse: Convert JSON response to a Python dictionary\n",
    "            data = response.json()\n",
    "            \n",
    "            # The 'features' list contains the actual sensor readings\n",
    "            readings = data['features']\n",
    "            \n",
    "            print(f\"\\n--- Live Sensor Update: {datetime.now().strftime('%H:%M:%S')} ---\")\n",
    "            print(f\"Total events detected in past hour: {data['metadata']['count']}\")\n",
    "            \n",
    "            # Display the first 3 most recent readings\n",
    "            for i, reading in enumerate(readings[:3]):\n",
    "                props = reading['properties']\n",
    "                magnitude = props['mag']\n",
    "                location = props['place']\n",
    "                print(f\"#{i+1}: Magnitude {magnitude} - Location: {location}\")\n",
    "        else:\n",
    "            print(\"Error: Could not connect to cloud.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Run the extraction loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Live Sensor Stream...\")\n",
    "    get_live_sensor_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86eaf2e3-8de6-435b-851c-91d89b7c6f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          type  units model_name version metrics.accuracy metrics.loss\n",
      "0   1     attention    512      GPT-X     4.0             0.95         0.05\n",
      "1   2  feed_forward   2048      GPT-X     4.0             0.95         0.05\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# collecting data in json format and also flattening it\n",
    "\n",
    "json_str = \"\"\"\n",
    "{\n",
    "    \"model_name\": \"GPT-X\",\n",
    "    \"version\": 4.0,\n",
    "    \"layers\": [\n",
    "        {\"id\": 1, \"type\": \"attention\", \"units\": 512},\n",
    "        {\"id\": 2, \"type\": \"feed_forward\", \"units\": 2048}\n",
    "    ],\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": 0.95,\n",
    "        \"loss\": 0.05\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(json_str)\n",
    "\n",
    "# Flatten 'layers' while keeping model-level fields\n",
    "df = pd.json_normalize(\n",
    "    data,\n",
    "    record_path=['layers'],\n",
    "    meta=['model_name', 'version', ['metrics', 'accuracy'], ['metrics', 'loss']]\n",
    "    )\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2443bc5a-fbdf-4cec-b573-256c47a02398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID      Feature  Value\n",
      "0  101  Temperature   23.5\n",
      "1  102     Humidity   45.2\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "# collecting data in xml format and also parsing it\n",
    "\n",
    "xml_data = \"\"\"\n",
    "<dataset>\n",
    "    <record>\n",
    "        <id>101</id>\n",
    "        <feature>Temperature</feature>\n",
    "        <value>23.5</value>\n",
    "    </record>\n",
    "    <record>\n",
    "        <id>102</id>\n",
    "        <feature>Humidity</feature>\n",
    "        <value>45.2</value>\n",
    "    </record>\n",
    "</dataset>\n",
    "\"\"\"\n",
    "\n",
    "# Parse XML\n",
    "root = ET.fromstring(xml_data)\n",
    "\n",
    "# Process: Extract data into a list of dictionaries\n",
    "parsed_data = []\n",
    "for record in root.findall('record'):\n",
    "    r_id = record.find('id').text\n",
    "    feature = record.find('feature').text\n",
    "    value = float(record.find('value').text)\n",
    "    parsed_data.append({\n",
    "        \"ID\": r_id,\n",
    "        \"Feature\": feature,\n",
    "        \"Value\": value\n",
    "    })\n",
    "\n",
    "# Convert into DataFrame\n",
    "df_xml = pd.DataFrame(parsed_data)\n",
    "\n",
    "print(df_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7616939-f6fc-48b8-b0cf-be266a112407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing browser...\n",
      "Accessing http://quotes.toscrape.com/js/...\n",
      "            Author                                              Quote\n",
      "0  Albert Einstein  “The world as we have created it is a process ...\n",
      "1     J.K. Rowling  “It is our choices, Harry, that show what we t...\n",
      "2  Albert Einstein  “There are only two ways to live your life. On...\n",
      "3      Jane Austen  “The person, be it gentleman or lady, who has ...\n",
      "4   Marilyn Monroe  “Imperfection is beauty, madness is genius and...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "# data collection from a dynamic website using Selenium\n",
    "\n",
    "url = \"http://quotes.toscrape.com/js/\"\n",
    "\n",
    "# 1. Setup Headless Chrome (Runs without a UI window)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\") \n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "data_selenium = []\n",
    "try:\n",
    "    # 2. Initialize the driver\n",
    "    print(\"Initializing browser...\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # 3. Load the page\n",
    "    print(f\"Accessing {url}...\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # 4. CRITICAL: Wait for JavaScript to load content\n",
    "    # We wait up to 10 seconds for the class 'quote' to appear\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"quote\"))\n",
    "    )\n",
    "    \n",
    "    # 5. Extract Data\n",
    "    quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "    \n",
    "    for q in quotes:\n",
    "        text = q.find_element(By.CLASS_NAME, \"text\").text\n",
    "        author = q.find_element(By.CLASS_NAME, \"author\").text\n",
    "        data_selenium.append({\"Author\": author, \"Quote\": text})\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    df_dynamic = pd.DataFrame(data_selenium)\n",
    "    print(df_dynamic.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Selenium Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # 6. Always quit the driver to free resources\n",
    "    if 'driver' in locals():\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d56c96-ee25-4cbb-946f-b4d72feef792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
